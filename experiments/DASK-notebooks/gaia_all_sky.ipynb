{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A demonstration of using Dask to visualize a density map of the full source catalog from the Gaia DR2 release"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "import holoviews.operation.datashader as hd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask provides utilities to build clusters to use in distributed compute jobs. In this particular case we will use a `KubeCluster`.  This is a type of cluster that knows how to use the kubernetes API to spawn separate pods for each worker.  The description for each worker is in a special file in the `dask` directory of the home directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, wait\n",
    "from dask import dataframe as dd\n",
    "from dask_kubernetes import KubeCluster\n",
    "import os\n",
    "cluster = KubeCluster.from_yaml(os.getenv('HOME') + '/dask/dask_worker.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are working in a dynamic environment where we don't know up front what ports/nodes our process will spawn on, there are a couple of utility classes that wrap dask classes to provide useful information about the running dask cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyterlabutils.notebook import LSSTDaskClient, ClusterProxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workers have the same profile as the instance selected to run the notebook.  For this example, we suggest a `large` size with 4 cores and 12GB of RAM.  Scaling to 60 threads is a good number for this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, os\n",
    "threads=60\n",
    "cpuper = float(os.getenv('CPU_LIMIT') or '4.0')\n",
    "workers = math.ceil(threads/cpuper)\n",
    "\n",
    "_ = cluster.scale_up(workers)\n",
    "client = LSSTDaskClient(address=cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The link above will take you to the status dashboard for the summary information about the cluster.\n",
    "\n",
    "The proxy in the next cell will give access to the status dashboards for each of the workers in your cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy = ClusterProxy(client)\n",
    "proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the following until you've got a full complement, or nearly so, of workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client)\n",
    "print(proxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now read the metadata for the parquet files we'll use for the analysis below.  This does not read all the data, but only the metadata for the files in this data set.\n",
    "\n",
    "> Note that either of the methods in the cell will work when runing at the LDF, however direct posix filesystem access may not work if running in a different environment (e.g. GKE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_parquet('/project/shared/data/gaia_dr2/gaia_source.parquet', columns=['l', 'b'], engine='fastparquet')\n",
    "#if reading from the cloud storage bucket, use the following instead\n",
    "#df = dd.read_parquet('gcs://jupyterlabdemo-gaia-dr2/gaia_source.parquet', columns=['l', 'b'], engine='fastparquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have asked for only a subset of the source catalog.  Specifically, only the galactic longitude and latitude.  We tell the dask client to cache these columns in memory on the worker nodes with the `persist` method to speed up computations in the future.\n",
    "> Note that the `persist` method is asynchronous, so following cells that interact with the dataframe may not execute until the persist is finished.  Follow along with progress by visiting the link in the output of cell 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now do things like count the number of rows.  This is still a parallel computation and you can look at the summary of the execution by going to the link in the output of cell 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to produce an aggregate map over cells on the sky.  The default is to simply count up the number of entries in each spacial cell.  To set up the color map, we ask that the smallest numbers be shown in light blue and the largest in darkblue with a linear ramp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client._timeout)\n",
    "print(client.overall_timeout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hd.shade.cmap=[\"lightblue\", \"darkblue\"]\n",
    "hv.extension(\"bokeh\", \"matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the points to be aggregated.  Defaults are fine here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = hv.Points(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the aggregation and display.  The `datashade` method will bin each of our two spacial coordinates and sum the entries in each.  This effectively produces a density map of the sky for all 1.7 billion entries in the Gaia DR2 source catalog.  Since we are using bokeh as the rendering library, the standard pan and zoom widgets are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%opts RGB [width=1000, height=500]\n",
    "hd.datashade(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close down the cluster\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
